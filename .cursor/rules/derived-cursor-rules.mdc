---
description: AI rules derived by SpecStory from the project AI interaction history
globs: *
---

## AI Coding Assistant Rules

## TECH STACK

- Node.js
- Prisma
- TypeScript
- Cloudflare Workers
- Wrangler
- SQLite
- `@prisma/adapter-d1`
- `dotenv`
- `hono`

## PROJECT DOCUMENTATION & CONTEXT SYSTEM

- All interactions with the AI coding assistant are logged and versioned (e.g., 2025-04-03_07-41-writing-csv-data-to-local-file.md).
- Version updates are noted in relevant sections (e.g., TECH STACK, WORKFLOW & RELEASE RULES).  Prisma version 6.6.0 is currently in use.  Wrangler version 4.10.0 is currently in use. A Prisma update to version 6.7.0 is available.  A Prisma update to 6.7.0 was noted on 2025-05-13.


## CODING STANDARDS

- Use consistent formatting (e.g., UTF-8 encoding for CSV files).
- Avoid global formatting of Markdown code blocks.  Use `$" ... "$` or HTML entity encoding (e.g., `&#44;`) to protect code blocks from unintended formatting by Markdown formatting plugins.
- When importing built-in Node.js modules, use the `node:` protocol (e.g., `import fs from 'node:fs';`).


## WORKFLOW & RELEASE RULES

- To deploy updated workers, use `pnpm wrangler deploy` after ensuring you are in the correct project directory (`cd apps/an-monkey-worker`).  This command should be run from the root of the `an-monkey-worker` project directory. Before deploying, ensure all dependencies are installed (`pnpm install`). Verify the `wrangler.jsonc` configuration is correct, especially the D1 database binding. After deployment, verify success by checking the deployment output and testing the API endpoints.
- Add the `"nodejs_compat"` flag to the `"compatibility_flags"` array in `wrangler.jsonc` to resolve compatibility issues with built-in Node.js modules.
- When deploying updates involving Prisma schema changes, ensure that the necessary database migrations are applied using `npx wrangler d1 execute an-monkey-db --file=./migrations/[migration_file_name].sql --remote`.  This is necessary when using Cloudflare D1 database.
- For initial database population with a JSON file (e.g., `work-data.json`), use a script with `npx wrangler d1 execute` and a carefully constructed SQL `INSERT` statement to handle data insertion efficiently. Consider batching for large datasets.
- **Deployment Steps (added 2025-05-13):**
    1. `cd apps/an-monkey-worker`
    2. `pnpm install`
    3. Check `wrangler.jsonc` configuration
    4. `pnpm run deploy`
    5. Verify API endpoint accessibility
    6. Execute database migration commands if necessary.
- When importing data, ensure date fields are correctly formatted as DATETIME for SQLite compatibility.  Incorrect formatting can lead to errors in date-based ordering.
- **Local Debugging of `/parttime` Worker (added 2025-05-13):** Ensure that the `where` and `orderBy` clauses in the `/parttime` route are correctly implemented to filter and sort results based on the `personnel` query parameter.  Review the code for any commented-out sections that might be affecting functionality. Use `pnpm run dev` to start the local development server.  Test the API endpoint at `http://localhost:8787/api/parttime?personnel=personnel_name`.
- **Database Migration (added 2025-05-13):** After schema changes, run `npx wrangler d1 execute an-monkey-db --file=./migrations/[migration_file_name].sql --remote` to apply migrations to the remote D1 database.  If using a local SQLite database for development, ensure the appropriate migration commands are executed for the local environment.  Verify migration success using `npx wrangler d1 execute an-monkey-db --command="SELECT name FROM sqlite_master WHERE type='table';" --remote` and `npx wrangler d1 execute an-monkey-db --command="PRAGMA table_info(PartTime);" --remote`. Restart the local development server (`pnpm run dev`) after applying migrations.
- **Route Path Definition (added 2025-05-13):**  Remove trailing slashes from route path definitions in router files (e.g., `part-time.ts`, `income.ts`, `word.ts`).  Use `${workspace}` instead of `${workspace}/` to avoid requiring trailing slashes in API endpoint URLs.
- **Applying Migrations (added 2025-05-13):**  Always apply database migrations after schema changes using `npx wrangler d1 execute an-monkey-db --file=./migrations/[migration_file_name].sql --remote`. Verify successful migration using `npx wrangler d1 execute an-monkey-db --command="SELECT name FROM sqlite_master WHERE type='table';" --remote` and `npx wrangler d1 execute an-monkey-db --command="PRAGMA table_info(PartTime);" --remote`. Restart the local development server after applying migrations.
- **Local Environment Setup (added 2025-05-13):** Create a `.env` file in the `apps/an-monkey-worker` directory to configure the local SQLite database for development.  This is necessary for local development and testing.  The `.env` file should contain the `DATABASE_URL` pointing to the local SQLite database file.  Example: `DATABASE_URL="file:./.wrangler/state/v3/d1/miniflare-D1DatabaseObject/1cea9eebaf31c8ae84ba91fbc0bba93dc7a57a1814bd437b6d7284abdb3b3115.sqlite"`
- **Local vs Remote Databases (added 2025-05-14):** Ensure that migrations are applied to both the local and remote databases.  The presence of data in the remote D1 database does not guarantee its presence in the local development database. Always run migration commands for both environments after schema changes.  Use `npx wrangler d1 execute an-monkey-db --file=./migrations/[migration_file_name].sql --local` for local migrations.
- **Local Database Migration (added 2025-05-14):** To ensure consistency between local and remote D1 databases, always apply migrations to both environments after schema changes. Use `npx wrangler d1 execute an-monkey-db --file=./migrations/[migration_file_name].sql --local` for local migrations and `npx wrangler d1 execute an-monkey-db --file=./migrations/[migration_file_name].sql --remote` for remote migrations.
- **Local Database Setup (added 2025-05-14):** To set up the local database, execute the following commands:
    1. `cd apps/an-monkey-worker`
    2. `echo 'DATABASE_URL="file:./.wrangler/state/v3/d1/miniflare-D1DatabaseObject/1cea9eebaf31c8ae84ba91fbc0bba93dc7a57a1814bd437b6d7284abdb3b3115.sqlite"' > .env`
    3. `npx wrangler d1 execute an-monkey-db --file=./migrations/add_part_time_model.sql --local`


## DEBUGGING

- When encountering TypeScript import errors, check for missing dependencies (e.g., `dotenv`) and correct import paths.  Use `npm list` to check package versions.
- Address type mismatches in Prisma configurations by reviewing the Prisma documentation for the correct types and configuration options for your Prisma version.  Consider upgrading Prisma when a compatible version is released.
- When dealing with errors related to column value mismatch in SQLite, verify that the number of values supplied matches the number of columns in the table.  Use explicit column names in INSERT statements and `json_extract` to map JSON fields to columns.
- If encountering "Property 'partTime' does not exist" errors in TypeScript, this is likely due to type checking issues after Prisma schema updates.  Regenerating the Prisma client (`npx prisma generate`) may resolve the issue; however, this does not affect the actual deployment.  This error may persist until the IDE or TypeScript server refreshes.
- If encountering errors related to ordering by date fields in the PartTime model, ensure the date field is correctly formatted as DATETIME in both the database and the import script. Consider using the timestamp field for ordering if date formatting issues persist.
- When encountering errors related to ordering by date fields in the PartTime model, ensure the `date` field is correctly formatted as DATETIME in the database and the import script.  If issues persist, use the `timestamp` field for ordering.
- When debugging, review the schema file (`schema.prisma`) and the relevant route files (e.g., `/parttime` route) for inconsistencies or errors.
- If encountering a "D1_ERROR: no such table: main.PartTime: SQLITE_ERROR", ensure the `PartTime` table has been created in the database by applying the necessary migration using `npx wrangler d1 execute an-monkey-db --file=./migrations/add_part_time_model.sql --remote`. Verify table creation with `npx wrangler d1 execute an-monkey-db --command="SELECT name FROM sqlite_master WHERE type='table';" --remote`.  Verify table structure with `npx wrangler d1 execute an-monkey-db --command="PRAGMA table_info(PartTime);" --remote`.  Restart the local development server after applying migrations.
- Add comprehensive logging statements (e.g., `console.log`) to track data flow and identify potential issues during development and debugging.
- When serializing data for JSON responses, ensure that BigInt types are converted to Number types to avoid serialization errors.  This can be done using `Number(BigIntValue)` within the relevant route handler.
- **BigInt Serialization (added 2025-05-14):** When working with BigInt types (like timestamps), ensure they are converted to Number types before serializing to JSON to avoid errors. Use `Number(BigIntValue)` for conversion.  If precision is critical, consider using a string conversion or a dedicated big number library.


## DATABASE SCHEMA & DATA HANDLING

- The `Income` table should have the following columns: `id` (TEXT, PRIMARY KEY), `owner` (TEXT, NOT NULL), `time` (DATETIME), `baseSalary` (REAL, NOT NULL), `overtimeMeal` (REAL, NOT NULL), `housingFund` (REAL, NOT NULL), `leaveDeduction` (REAL, NOT NULL), `housingFundDeduction` (REAL, NOT NULL), `medicalInsurance` (REAL, NOT NULL), `pensionInsurance` (REAL, NOT NULL), `unemploymentInsurance` (REAL, NOT NULL), `tax` (REAL, NOT NULL), `rent` (REAL, NOT NULL), `createdAt` (DATETIME, DEFAULT CURRENT_TIMESTAMP), `updatedAt` (DATETIME, DEFAULT CURRENT_TIMESTAMP). An index should be added on the `time` column.
- Use `DateTime` type for the `time` field in the Prisma schema for easier sorting and querying.
- When importing JSON data into Cloudflare D1, use Wrangler's `d1 execute` command with a carefully constructed SQL statement to ensure correct field mapping.  Consider batching large imports for better performance.  A sample command is:
```bash
npx wrangler d1 execute an-monkey-db \
  --command="INSERT INTO Income (id, owner, time, baseSalary, overtimeMeal, housingFund, leaveDeduction, housingFundDeduction, medicalInsurance, pensionInsurance, unemploymentInsurance, tax, rent, createdAt, updatedAt) 
  SELECT 
    json_extract(value, '$.id'),
    json_extract(value, '$.owner'),
    json_extract(value, '$.time'),
    json_extract(value, '$.baseSalary'),
    json_extract(value, '$.overtimeMeal'),
    json_extract(value, '$.housingFund'),
    json_extract(value, '$.leaveDeduction'),
    json_extract(value, '$.housingFundDeduction'),
    json_extract(value, '$.medicalInsurance'),
    json_extract(value, '$.pensionInsurance'),
    json_extract(value, '$.unemploymentInsurance'),
    json_extract(value, '$.tax'),
    json_extract(value, '$.rent'),
    json_extract(value, '$.createdAt'),
    json_extract(value, '$.updatedAt')
  FROM json_each('$(cat processed-income-data.json)')" --remote
```
- Add a new model, `PartTime`, to record part-time work hours.  The data structure should be: `{ "date": "YYYY-MM-DD", "timestamp": (unix timestamp in milliseconds), "hours": (number), "project": (string), "description": (string), "personnel": (string) }`.  The `PartTime` table should include indexes on `personnel` and `date` columns for optimized querying. Implement a server route similar to `/income` for querying `PartTime` data.  The `/parttime` endpoint should accept a `personnel` query parameter and return data sorted by `date` (descending) or `timestamp` (descending) as a fallback.  For initial data population of the `PartTime` table, a SQL script should be used with `npx wrangler d1 execute` for efficient batch insertion.  Ensure that the `date` field is correctly formatted as DATETIME in the database and import scripts.  The `date` field in the `PartTime` model is now a `String` type to match the data import format.  For improved performance and to avoid BigInt serialization issues, consider using an `Int` type for the `timestamp` field instead of `BigInt`.


## API ENDPOINTS

- `/word`: Existing endpoint for word-related operations.
- `/income`: New endpoint to query income data. Accepts `owner` query parameter, returns data sorted by `time` (descending).
- `/parttime`: New endpoint to query part-time work hour data. Accepts `personnel` query parameter, returns data sorted by `date` (descending), or `timestamp` (descending) if date sorting fails.


## BEST PRACTICES

- Always validate user inputs to prevent errors and security vulnerabilities.
- Handle errors gracefully and provide informative error messages.
- Use appropriate data types for database fields.  Consider using `Int` instead of `BigInt` for timestamps that fall within the JavaScript Number safe integer range.
- Add indexes to frequently queried fields to improve performance.
- Consider using batch operations for large data imports.
- Test changes thoroughly before deploying to production.
- Ensure date fields are correctly formatted as DATETIME in database and import scripts to avoid ordering errors.  Use the timestamp field as a fallback for ordering if date formatting issues persist.